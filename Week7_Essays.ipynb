{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQz8ksTDKJuaZzi9eZhHpc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdulmuj33b/Python_AI_and_ML-WK7/blob/main/Week7_Essays.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Q1: Define algorithmic bias and provide two examples of how it manifests in AI systems.*\n",
        "\n",
        "Algorithmic bias occurs when an AI system produces unfair, discriminatory outcomes, often against marginalized groups, due to flawed assumptions or data in its development.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Hiring Algorithms: A resume-screening tool trained on historical data from male-dominated industries may downgrade resumes with words like \"women's college\" or prioritize male-associated traits.\n",
        "\n",
        "Predictive Policing: Systems trained on biased arrest data may disproportionately flag neighborhoods with higher minority populations, reinforcing over-policing cycles.\n",
        "\n"
      ],
      "metadata": {
        "id": "XCNnuK9BoZxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Explain the difference between transparency and explainability in AI. Why are both important?\n",
        "\n",
        "- Transparency: Openness about how an AI system is built (data sources, model design, testing methods).\n",
        "\n",
        "- Explainability: The ability to clarify why a specific decision/output was generated in understandable terms.\n",
        "\n",
        "Importance:\n",
        "\n",
        "- Transparency enables accountability, audits, and trust from regulators/stakeholders.\n",
        "\n",
        "- Explainability allows users to challenge decisions, builds trust, and ensures compliance (e.g., GDPR’s \"right to explanation\").\n",
        "Both are crucial for ethical deployment, debugging bias, and user empowerment.\n",
        "\n"
      ],
      "metadata": {
        "id": "-01YoUVvpaJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Q3: How does GDPR impact AI development in the EU?*\n",
        "\n",
        "GDPR imposes key constraints on AI development:\n",
        "\n",
        "- Lawfulness & Consent: Requires explicit consent for personal data use in training/models.\n",
        "\n",
        "- Purpose Limitation: Data collected for one purpose cannot be reused for AI training without new consent.\n",
        "\n",
        "- Right to Explanation: Users can demand reasoning behind automated decisions affecting them.\n",
        "\n",
        "- Data Minimization: Only essential data can be processed, limiting training datasets.\n",
        "\n",
        "- Bias Mitigation: Mandates proactive steps to avoid discriminatory outcomes.\n",
        "\n",
        "- Fines: Non-compliance risks penalties up to 4% of global revenue.\n",
        "Result: Forces developers to prioritize privacy, fairness, and accountability.\n",
        "\n"
      ],
      "metadata": {
        "id": "MMsh0FIysUDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Justice (A) refers to the fair distribution of AI's benefits and risks across all groups in society, ensuring equitable access and preventing systemic discrimination.\n",
        "\n",
        "Non-maleficence (B) mandates that AI systems must avoid causing harm—whether physical, psychological, social, or economic—to individuals or communities.\n",
        "\n",
        "Autonomy (C) emphasizes respecting users' rights to self-determination, including informed consent, control over personal data, and the ability to override AI decisions affecting them.\n",
        "\n",
        "Sustainability (D) requires designing, deploying, and operating AI systems in ways that minimize environmental impact (e.g., energy efficiency) and promote long-term ecological balance.\n",
        "\n",
        "Collectively, these four pillars form the ethical backbone of trustworthy AI:\n",
        "\n",
        "- Non-maleficence establishes the baseline: \"First, do no harm.\"\n",
        "\n",
        "- Autonomy protects human agency in an automated world.\n",
        "\n",
        "- Justice ensures fairness in outcomes.\n",
        "\n",
        "- Sustainability addresses the planetary footprint of AI infrastructure."
      ],
      "metadata": {
        "id": "zvRlD0PHsjc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PART 2"
      ],
      "metadata": {
        "id": "zpCtuDVdARpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Case Study Critique: Amazon hiring tool"
      ],
      "metadata": {
        "id": "lThkOX3AukrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Source of Bias"
      ],
      "metadata": {
        "id": "9JO3ep2PCdzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bias stemmed from flawed training data and problematic model design:\n",
        "\n",
        "- Training Data: The AI was trained on 10 years of male-dominated tech industry resumes (primarily submitted by men). This taught the system to associate \"successful candidate\" patterns with male-centric language (e.g., verbs like \"executed\"), schools, or hobbies.\n",
        "\n",
        "- Model Design:\n",
        "\n",
        "    - It penalized resumes containing words like \"women’s\" (e.g., \"women’s chess club captain\").\n",
        "\n",
        "  - It rewarded male-associated terms (e.g., \"captain of football team\").\n",
        "\n",
        "  - No debiasing techniques (e.g., fairness constraints) were implemented.\n",
        "\n",
        "Result: The tool downgraded female applicants, reinforcing historical gender disparities."
      ],
      "metadata": {
        "id": "4WNpshOxCiHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Solution**                    | **Implementation**                                                                                                                                       |\n",
        "|--------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **A. Curate Representative Training Data** | • Oversample resumes from women/non-binary candidates.<br>• Remove gender proxies (e.g., names, gender-linked clubs).<br>• Synthesize balanced data using generative models. |\n",
        "| **B. Algorithmic Fairness Constraints**    | • Apply demographic parity (equal selection rates across genders) or equal opportunity (equal true positive rates).<br>• Use adversarial debiasing to remove gender correlations from embeddings. |\n",
        "| **C. Human-AI Hybrid Workflow**            | • AI screens for skills only (e.g., Python, project management), not \"culture fit.\"<br>• Human reviewers audit AI-rejected resumes.<br>• Mandatory diversity reviews for shortlisted candidates. |\n"
      ],
      "metadata": {
        "id": "q8MUMe-eBE_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Fairness Evaluation Metrics\n",
        "Quantitative Metrics:"
      ],
      "metadata": {
        "id": "_60cMl9NCaud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Metric**             | **Formula / Purpose**                                                                 |\n",
        "|------------------------|----------------------------------------------------------------------------------------|\n",
        "| **Demographic Parity** | Selection Rate (Women) = Selection Rate (Men)<br>→ Avoids disparate impact             |\n",
        "| **Equal Opportunity**  | True Positive Rate (Women) = TPR (Men)<br>→ Ensures qualified candidates aren’t overlooked |\n",
        "| **Predictive Parity**  | PPV (Women) = PPV (Men)<br>→ Ensures selected candidates succeed at equal rates        |\n"
      ],
      "metadata": {
        "id": "oMchhywjB1g2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qualitative Checks:\n",
        "\n",
        "Bias Audits: Test tool with resumes where only gender pronouns/terms differ.\n",
        "\n",
        "Long-term Impact Tracking: Monitor hiring diversity 1–3 years post-implementation.\n",
        "\n",
        "User Feedback: Survey rejected candidates about perceived fairness."
      ],
      "metadata": {
        "id": "px682uEZCMi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amazon’s case exemplifies how unchecked historical data amplifies societal biases. Effective solutions require both technical interventions (data/model fixes) and human oversight to ensure equitable outcomes."
      ],
      "metadata": {
        "id": "-qLQ4C6MDOBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "grmG9eouDuJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ethical Risks of Biased Facial Recognition\n",
        "- Wrongful Arrests & Detentions\n",
        "\n",
        "  Example: Higher false positive rates for minorities lead to innocent people being arrested (e.g., Robert Williams in Detroit).\n",
        "\n",
        "  Impact: Trauma, legal costs, loss of employment, and eroded trust in law enforcement.\n",
        "\n",
        "- Reinforcement of Systemic Discrimination\n",
        "\n",
        "  Over-policing of minority neighborhoods generates biased training data, creating a feedback loop of injustice.\n",
        "\n",
        "- Mass Surveillance & Privacy Erosion\n",
        "\n",
        "  Continuous public monitoring violates autonomy and enables authoritarian control.\n",
        "\n",
        "- Chilling Effect on Civil Liberties\n",
        "\n",
        "  Fear of being tracked may suppress participation in protests or community gatherings.\n",
        "\n",
        "- Lack of Recourse\n",
        "\n",
        "  Victims struggle to challenge opaque AI decisions due to limited explainability\n",
        "\n"
      ],
      "metadata": {
        "id": "dscU6b7qDwNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Recommended Policies for Responsible Deployment\n",
        "\n",
        "###A. Regulatory Safeguards"
      ],
      "metadata": {
        "id": "hALnWFAOElMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Policy**                     | **Implementation**                                                                                                                                     |\n",
        "|-------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **1. Ban High-Risk Use Cases** | Prohibit facial recognition for:<br>• Real-time mass surveillance<br>• Sole evidence for arrests                                                        |\n",
        "| **2. Accuracy & Bias Standards** | Mandate:<br>• 99.9% accuracy across all demographic groups<br>• Regular third-party audits (e.g., NIST standards)                                       |\n",
        "| **3. Warrant Requirements**    | Require court-approved warrants for targeted searches with strict scope/duration limits                                                                 |\n"
      ],
      "metadata": {
        "id": "3eR6iNNqEtp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Operational Policies\n",
        "\n",
        "| **Policy**                      | **Implementation**                                                                                                                                       |\n",
        "|--------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **4. Human Oversight Protocol** | • AI outputs treated only as investigative leads<br>• Require 2+ human reviewers for identification<br>• No action without corroborating evidence         |\n",
        "| **5. Transparency & Accountability** | • Public disclosure of deployment locations/accuracy rates<br>• Independent review boards for misuse investigations                                     |\n",
        "| **6. Redress Mechanisms**       | • Clear process for victims to challenge misidentifications<br>• Mandatory compensation for wrongful arrests                                             |\n"
      ],
      "metadata": {
        "id": "x-YVPaAZFH69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C. Technical & Training Policies\n",
        "\n",
        "| **Policy**             | **Implementation**                                                                                                                   |\n",
        "|------------------------|--------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **7. Bias Mitigation** | • Diverse training data (age, gender, skin tone)<br>• Adversarial debiasing algorithms<br>• Continuous performance monitoring         |\n",
        "| **8. Officer Training**| • Mandatory education on system limitations and bias risks<br>• Protocols for ethical deployment                                     |\n"
      ],
      "metadata": {
        "id": "j0e_ZVmBFfr2"
      }
    }
  ]
}